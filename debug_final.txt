
======================================================================
   Hybrid QMolNet: Quantum-Classical Drug Molecule Prediction
======================================================================

Configuration:
  Samples:      100
  Epochs:       10
  Batch size:   32
  Train quantum:True
  Output dir:   outputs

[1/7] Importing modules...
Random seed set to 42 for reproducibility
Using CPU device

[2/7] Preparing dataset...
Generating synthetic dataset for demonstration...
Dataset size: 100 molecules
Class distribution: 50 positive, 50 negative

Dataset splits:
  Train: 69 molecules
  Val:   16 molecules
  Test:  15 molecules

Processing training set...
Processing 69 molecules...

Building graphs:   0%|          | 0/69 [00:00<?, ?it/s]
Building graphs: 100%|##########| 69/69 [00:00<00:00, 1210.31it/s]
Successfully processed 69/69 molecules

Processing validation set...
Processing 16 molecules...

Building graphs:   0%|          | 0/16 [00:00<?, ?it/s]
Building graphs: 100%|##########| 16/16 [00:00<00:00, 1363.50it/s]
Successfully processed 16/16 molecules

Processing test set...
Processing 15 molecules...

Building graphs:   0%|          | 0/15 [00:00<?, ?it/s]
Building graphs: 100%|##########| 15/15 [00:00<00:00, 1426.28it/s]
Successfully processed 15/15 molecules
Processing 100 molecules...

Building graphs:   0%|          | 0/100 [00:00<?, ?it/s]
Building graphs: 100%|##########| 100/100 [00:00<00:00, 1320.55it/s]
Successfully processed 100/100 molecules

Node feature dimension: 145

[3/7] Training GNN Baseline...

============================================================
GNN Classifier Summary
============================================================
GNNClassifier(
  node_features=145,
  embedding_dim=32,
  num_classes=2
)
------------------------------------------------------------
Total parameters:     32,706
Trainable parameters: 32,706
============================================================


============================================================
Training GNN_Baseline
============================================================
Device: cpu
Epochs: 10
Train batches: 3
Val batches: 1

Epoch   1/10 | Train Loss: 0.6925 | Train Acc: 0.5072 | Val Loss: 0.6932 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6932
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch0_val_loss_0.6932.pt
Epoch   2/10 | Train Loss: 0.6921 | Train Acc: 0.5507 | Val Loss: 0.6932 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] No improvement for 1/10 epochs
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch1_val_loss_0.6932.pt
Epoch   3/10 | Train Loss: 0.6802 | Train Acc: 0.6522 | Val Loss: 0.6931 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6931
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch2_val_loss_0.6931.pt
Epoch   4/10 | Train Loss: 0.6834 | Train Acc: 0.6522 | Val Loss: 0.6928 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6928
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch3_val_loss_0.6928.pt
Epoch   5/10 | Train Loss: 0.6719 | Train Acc: 0.6087 | Val Loss: 0.6926 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6926
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch4_val_loss_0.6926.pt
Epoch   6/10 | Train Loss: 0.6629 | Train Acc: 0.7246 | Val Loss: 0.6912 | Val Acc: 0.6875 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6912
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch5_val_loss_0.6912.pt
Epoch   7/10 | Train Loss: 0.6416 | Train Acc: 0.7101 | Val Loss: 0.6885 | Val Acc: 0.6875 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6885
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch6_val_loss_0.6885.pt
Epoch   8/10 | Train Loss: 0.6194 | Train Acc: 0.7101 | Val Loss: 0.6836 | Val Acc: 0.7500 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6836
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch7_val_loss_0.6836.pt
Epoch   9/10 | Train Loss: 0.5775 | Train Acc: 0.7681 | Val Loss: 0.6745 | Val Acc: 0.8125 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6745
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch8_val_loss_0.6745.pt
Epoch  10/10 | Train Loss: 0.5377 | Train Acc: 0.8696 | Val Loss: 0.6552 | Val Acc: 0.8125 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6552
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch9_val_loss_0.6552.pt

Training complete!
Best validation loss: 0.6552 at epoch 10
Best validation accuracy: 0.8125
Training curves saved to outputs\figures\gnn_training.png

[4/7] Training Descriptor MLP Baseline...
Computing molecular descriptors...

============================================================
Descriptor MLP Summary
============================================================
DescriptorMLP(
  input_dim=10,
  hidden_dims=(64, 32, 16),
  num_classes=2
)
------------------------------------------------------------
Total parameters:     3,570
Trainable parameters: 3,570
============================================================


Training Descriptor_MLP...
Epoch  10/10 | Train Loss: 0.3982 | Val Loss: 0.4113 | Val Acc: 0.8000

Training complete!
Best val loss: 0.4113 at epoch 10
Training curves saved to outputs\figures\mlp_training.png

[5/7] Training Hybrid Quantum-Classical Model...

======================================================================
Hybrid QMolNet Architecture Summary
======================================================================

[Pipeline Overview]
+-----------------------------------------------------------------+
|  SMILES -> Graph -> GNN -> Compress -> [Quantum] -> Classifier  |
|                    |         |          |            |          |
|               145D ->  32D ->    8D  ->      8D  ->       2    |
+-----------------------------------------------------------------+

[Component Details]
----------------------------------------------------------------------

1. GNN Encoder (28,448 params)
   Input: 145 node features
   Output: 32-dim graph embedding
   Layers: 3 GCN layers

2. Compression Layer (280 params)
   Linear: 32 -> 8
   LayerNorm + Tanh activation

3. Quantum Layer (72 params)
   Qubits: 8
   Variational layers: 3
   Encoding: RY angle encoding
   Entanglement: CNOT ring
   Measurement: Pauli-Z expectations

4. Classifier Head (178 params)
   MLP: 8 -> 16 -> 2

----------------------------------------------------------------------

Total Parameters: 28,978
  Classical: 28,906
  Quantum:   72
======================================================================


============================================================
Training Hybrid_QMolNet
============================================================
Device: cpu
Epochs: 10
Train batches: 3
Val batches: 1

Traceback (most recent call last):
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\run_all.py", line 449, in <module>
    main()
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\run_all.py", line 279, in main
    hybrid_history = hybrid_trainer.fit(
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\training\trainer.py", line 267, in fit
    train_loss, train_acc = self.train_epoch(train_loader)
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\training\trainer.py", line 172, in train_epoch
    logits = self.model.forward_batch(batch)
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\models\hybrid_model.py", line 165, in forward_batch
    return self.forward(data.x, data.edge_index, data.batch)
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\models\hybrid_model.py", line 143, in forward
    quantum_out = self.quantum_layer(compressed_cpu)  # [B, 8]
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\.venv\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\models\quantum_layer.py", line 194, in forward
    out = self.forward_single(x[i])
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\models\quantum_layer.py", line 175, in forward_single
    return torch.stack(expectations)
TypeError: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor
