
======================================================================
   Hybrid QMolNet: Quantum-Classical Drug Molecule Prediction
======================================================================

Configuration:
  Samples:      100
  Epochs:       10
  Batch size:   32
  Train quantum:True
  Output dir:   outputs

[1/7] Importing modules...
Random seed set to 42 for reproducibility
Using CPU device

[2/7] Preparing dataset...
Generating synthetic dataset for demonstration...
Dataset size: 100 molecules
Class distribution: 50 positive, 50 negative

Dataset splits:
  Train: 69 molecules
  Val:   16 molecules
  Test:  15 molecules

Processing training set...
Processing 69 molecules...

Building graphs:   0%|          | 0/69 [00:00<?, ?it/s]
Building graphs: 100%|##########| 69/69 [00:00<00:00, 939.01it/s]
Successfully processed 69/69 molecules

Processing validation set...
Processing 16 molecules...

Building graphs:   0%|          | 0/16 [00:00<?, ?it/s]
Building graphs: 100%|##########| 16/16 [00:00<00:00, 1326.37it/s]
Successfully processed 16/16 molecules

Processing test set...
Processing 15 molecules...

Building graphs:   0%|          | 0/15 [00:00<?, ?it/s]
Building graphs: 100%|##########| 15/15 [00:00<00:00, 1302.39it/s]
Successfully processed 15/15 molecules
Processing 100 molecules...

Building graphs:   0%|          | 0/100 [00:00<?, ?it/s]
Building graphs: 100%|##########| 100/100 [00:00<00:00, 1603.11it/s]
Traceback (most recent call last):
Successfully processed 100/100 molecules

Node feature dimension: 145

[3/7] Training GNN Baseline...

============================================================
GNN Classifier Summary
============================================================
GNNClassifier(
  node_features=145,
  embedding_dim=32,
  num_classes=2
)
------------------------------------------------------------
Total parameters:     32,706
Trainable parameters: 32,706
============================================================


============================================================
Training GNN_Baseline
============================================================
Device: cpu
Epochs: 10
Train batches: 3
Val batches: 1

Epoch   1/10 | Train Loss: 0.6925 | Train Acc: 0.5072 | Val Loss: 0.6932 | Val Acc: 0.5000 | Time: 0.1s
  [EarlyStopping] New best val_loss: 0.6932
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch0_val_loss_0.6932.pt
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\run_all.py", line 458, in <module>
Epoch   2/10 | Train Loss: 0.6921 | Train Acc: 0.5507 | Val Loss: 0.6932 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] No improvement for 1/10 epochs
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch1_val_loss_0.6932.pt
Epoch   3/10 | Train Loss: 0.6802 | Train Acc: 0.6522 | Val Loss: 0.6931 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6931
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch2_val_loss_0.6931.pt
Epoch   4/10 | Train Loss: 0.6834 | Train Acc: 0.6522 | Val Loss: 0.6928 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6928
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch3_val_loss_0.6928.pt
Epoch   5/10 | Train Loss: 0.6719 | Train Acc: 0.6087 | Val Loss: 0.6926 | Val Acc: 0.5000 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6926
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch4_val_loss_0.6926.pt
Epoch   6/10 | Train Loss: 0.6629 | Train Acc: 0.7246 | Val Loss: 0.6912 | Val Acc: 0.6875 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6912
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch5_val_loss_0.6912.pt
Epoch   7/10 | Train Loss: 0.6416 | Train Acc: 0.7101 | Val Loss: 0.6885 | Val Acc: 0.6875 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6885
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch6_val_loss_0.6885.pt
Epoch   8/10 | Train Loss: 0.6194 | Train Acc: 0.7101 | Val Loss: 0.6836 | Val Acc: 0.7500 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6836
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch7_val_loss_0.6836.pt
Epoch   9/10 | Train Loss: 0.5775 | Train Acc: 0.7681 | Val Loss: 0.6745 | Val Acc: 0.8125 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6745
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch8_val_loss_0.6745.pt
Epoch  10/10 | Train Loss: 0.5377 | Train Acc: 0.8696 | Val Loss: 0.6552 | Val Acc: 0.8125 | Time: 0.0s
  [EarlyStopping] New best val_loss: 0.6552
  [Checkpoint] Saved to outputs\checkpoints\checkpoint_epoch9_val_loss_0.6552.pt

Training complete!
Best validation loss: 0.6552 at epoch 10
Best validation accuracy: 0.8125
Training curves saved to outputs\figures\gnn_training.png

[4/7] Training Descriptor MLP Baseline...
Computing molecular descriptors...
DEBUG: Stacking 100 items. Type of first: <class 'torch.Tensor'>

============================================================
Descriptor MLP Summary
============================================================
DescriptorMLP(
  input_dim=10,
  hidden_dims=(64, 32, 16),
  num_classes=2
)
------------------------------------------------------------
Total parameters:     3,570
Trainable parameters: 3,570
============================================================


Training Descriptor_MLP...
Epoch  10/10 | Train Loss: 0.3982 | Val Loss: 0.4113 | Val Acc: 0.8000

Training complete!
Best val loss: 0.4113 at epoch 10
Training curves saved to outputs\figures\mlp_training.png

[5/7] Training Hybrid Quantum-Classical Model...

======================================================================
Hybrid QMolNet Architecture Summary
======================================================================

[Pipeline Overview]
    main()
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\run_all.py", line 273, in main
    print_hybrid_model_summary(hybrid_model)
  File "C:\Users\ashmi\.gemini\antigravity\scratch\hybrid_qmolnet\models\hybrid_model.py", line 244, in print_hybrid_model_summary
    print("\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510")
  File "C:\Users\ashmi\AppData\Local\Programs\Python\Python310\lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 0-66: character maps to <undefined>
